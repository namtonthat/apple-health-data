"""
Health & Fitness Dashboard

Streamlit dashboard for visualizing health data from Apple Health
and workout data from Hevy.

Reads from S3 transformed/ parquet files generated by dbt.
"""

import os
from datetime import date, timedelta
from pathlib import Path

import duckdb
import polars as pl
import streamlit as st

# Page config
st.set_page_config(
    page_title="Health Dashboard",
    page_icon="ðŸ’ª",
    layout="wide",
)

# Load environment
from dotenv import load_dotenv
load_dotenv(Path(__file__).parent.parent.parent / ".env")

# S3 configuration
S3_BUCKET = os.environ.get("S3_BUCKET_NAME", "")
S3_TRANSFORMED_PREFIX = "transformed"


@st.cache_resource
def get_connection():
    """Get DuckDB connection configured for S3 access."""
    conn = duckdb.connect(":memory:")

    # Get credentials from environment (bucket is in ap-southeast-2)
    region = "ap-southeast-2"  # Hardcoded - bucket region
    access_key = os.environ.get("AWS_ACCESS_KEY_ID", "")
    secret_key = os.environ.get("AWS_SECRET_ACCESS_KEY", "")

    # Configure S3 credentials
    conn.execute(f"SET s3_region = '{region}'")
    conn.execute(f"SET s3_access_key_id = '{access_key}'")
    conn.execute(f"SET s3_secret_access_key = '{secret_key}'")

    return conn


def get_s3_path(table_name: str) -> str:
    """Get S3 path for a transformed table."""
    # dbt-duckdb writes single parquet files without extension
    return f"s3://{S3_BUCKET}/{S3_TRANSFORMED_PREFIX}/{table_name}"


def load_daily_summary(start_date: date, end_date: date) -> pl.DataFrame:
    """Load daily summary data for date range from S3."""
    conn = get_connection()
    s3_path = get_s3_path("fct_daily_summary")
    query = f"""
        SELECT *
        FROM read_parquet('{s3_path}')
        WHERE date BETWEEN ? AND ?
        ORDER BY date
    """
    try:
        return pl.from_arrow(conn.execute(query, [start_date, end_date]).fetch_arrow_table())
    except Exception as e:
        if "No files found" in str(e):
            return pl.DataFrame()
        raise


def load_workout_sets(start_date: date, end_date: date) -> pl.DataFrame:
    """Load workout sets for date range from S3."""
    conn = get_connection()
    s3_path = get_s3_path("fct_workout_sets")
    query = f"""
        SELECT
            workout_date,
            workout_name,
            exercise_name,
            set_number,
            weight_kg,
            reps,
            volume_kg,
            rpe,
            set_type,
            started_at,
            exercise_order
        FROM read_parquet('{s3_path}')
        WHERE workout_date BETWEEN ? AND ?
        ORDER BY workout_date DESC, started_at DESC, exercise_order, set_number
    """
    try:
        return pl.from_arrow(conn.execute(query, [start_date, end_date]).fetch_arrow_table())
    except Exception as e:
        if "No files found" in str(e):
            return pl.DataFrame()
        raise


# Sidebar - Date Filter
st.sidebar.title("Filters")

# Quick date range presets
preset = st.sidebar.radio(
    "Date Range",
    ["Last 7 days", "Last 30 days", "This month", "Custom"],
    index=0,
)

today = date.today()
if preset == "Last 7 days":
    start_date = today - timedelta(days=7)
    end_date = today
elif preset == "Last 30 days":
    start_date = today - timedelta(days=30)
    end_date = today
elif preset == "This month":
    start_date = today.replace(day=1)
    end_date = today
else:
    start_date = st.sidebar.date_input("Start date", today - timedelta(days=7))
    end_date = st.sidebar.date_input("End date", today)

st.sidebar.markdown(f"**Showing:** {start_date} to {end_date}")

# Store dates in session state for pages to access
st.session_state["start_date"] = start_date
st.session_state["end_date"] = end_date

# Main content - Home page
st.title("Health & Fitness Dashboard")
st.markdown("""
Use the sidebar to navigate between pages:

- **Recovery & Health** - Sleep tracking and nutrition data
- **Exercises** - Workout logs and 1RM tracking
""")

# Quick summary
df_daily = load_daily_summary(start_date, end_date)
df_exercises = load_workout_sets(start_date, end_date)

if df_daily.height == 0 and df_exercises.height == 0:
    st.error("""
    **No data found in S3 transformed/**

    Run the pipeline first to populate the data:
    ```bash
    ./scripts/run_pipeline.sh
    ```
    """)
else:
    col1, col2, col3 = st.columns(3)
    with col1:
        if "sleep_hours" in df_daily.columns:
            avg_sleep = df_daily["sleep_hours"].drop_nulls().mean()
            st.metric("Avg Sleep", f"{avg_sleep:.1f}h" if avg_sleep else "-")
    with col2:
        if "protein_g" in df_daily.columns:
            avg_protein = df_daily["protein_g"].drop_nulls().mean()
            st.metric("Avg Protein", f"{avg_protein:.0f}g" if avg_protein else "-")
    with col3:
        if df_exercises.height > 0:
            n_workouts = df_exercises["workout_date"].n_unique()
            st.metric("Workouts", n_workouts)
